{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "pd.set_option(\"display.width\", 500)\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing your CSV files\n",
    "csv_directory = \"../data/\"\n",
    "\n",
    "# Define the output directory for user action files\n",
    "output_directory = \"../data_users/\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Load user IDs from Parquet files\n",
    "super_users_df = pd.read_parquet(\"../data_users/SuperUserIds.parquet\")\n",
    "# non_super_users_df = pd.read_parquet(\"../data_users/NonSuperUserIds.parquet\")\n",
    "\n",
    "# Convert DataFrames to sets for faster lookup\n",
    "super_user_ids = set(super_users_df[\"Id\"].astype(int))\n",
    "# non_super_user_ids = set(non_super_users_df[\"Id\"].astype(int))\n",
    "\n",
    "# Mapping of CSV files to user ID columns and action types\n",
    "file_mappings = {\n",
    "    \"Votes.csv\": {\n",
    "        \"user_id_column\": \"UserId\",\n",
    "        \"action_type_column\": \"VoteTypeId\",\n",
    "        \"action_type_map\": {2: \"UpVote\", 3: \"DownVote\"},\n",
    "        \"date_column\": \"CreationDate\",\n",
    "    },\n",
    "    \"Posts.csv\": {\n",
    "        \"user_id_column\": \"OwnerUserId\",\n",
    "        \"action_type_column\": \"PostTypeId\",\n",
    "        \"action_type_map\": {1: \"Question\", 2: \"Answer\"},\n",
    "        \"date_column\": \"CreationDate\",\n",
    "    },\n",
    "    \"Comments.csv\": {\n",
    "        \"user_id_column\": \"UserId\",\n",
    "        \"action_type\": \"Comment\",\n",
    "        \"date_column\": \"CreationDate\",\n",
    "    },\n",
    "    \"Badges.csv\": {\n",
    "        \"user_id_column\": \"UserId\",\n",
    "        \"action_type\": \"Badge\",\n",
    "        \"date_column\": \"Date\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define the columns to extract for each file\n",
    "columns_mapping = {\n",
    "    \"Posts.csv\": [\"OwnerUserId\", \"CreationDate\", \"PostTypeId\"],\n",
    "    \"Comments.csv\": [\"UserId\", \"CreationDate\"],\n",
    "    \"Votes.csv\": [\"UserId\", \"CreationDate\", \"VoteTypeId\"],\n",
    "    \"Badges.csv\": [\"UserId\", \"Date\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   BountyAmount             CreationDate        Id  PostId  UserId  VoteTypeId\n",
      "0           NaN  2011-05-12T00:00:00.000  14967360    8440     NaN           2\n",
      "1           NaN  2011-05-12T00:00:00.000  14969494    8440     NaN           2\n",
      "2           NaN  2011-05-14T00:00:00.000  15032293    8440     NaN           2\n",
      "3           NaN  2011-05-16T00:00:00.000  15062845    8440     NaN           2\n",
      "4           NaN  2011-05-25T00:00:00.000  15350663    8440     NaN           2\n"
     ]
    }
   ],
   "source": [
    "# testing - reading the first 25% of Vots.csv and printing valuecounts of VoteTypeId\n",
    "votes_df = pd.read_csv(\"../data/Votes.csv\", nrows=1000000, delimiter=\"\\x17\")\n",
    "print(votes_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_total_chunks(file_path, chunksize, sample_size_bytes=1024*1024):\n",
    "    total_bytes = os.path.getsize(file_path)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        sample_bytes = f.read(sample_size_bytes)\n",
    "    sample_lines = sample_bytes.count(b'\\n') or 1  # Avoid division by zero\n",
    "    avg_bytes_per_line = sample_size_bytes / sample_lines\n",
    "    estimated_total_lines = total_bytes / avg_bytes_per_line\n",
    "    total_chunks = estimated_total_lines / chunksize\n",
    "    return int(total_chunks)\n",
    "\n",
    "def process_file(file_name, mapping, columns, user_ids, output_file, chunksize=10**6):\n",
    "    file_path = os.path.join(csv_directory, file_name)\n",
    "    print(f\"Processing {file_name}...\")\n",
    "    date_column = mapping.get('date_column', 'CreationDate')\n",
    "\n",
    "    # Estimate total chunks\n",
    "    total_chunks = estimate_total_chunks(file_path, chunksize)\n",
    "    print(f\"Estimated total chunks: {total_chunks}\")\n",
    "\n",
    "    # Read the CSV in chunks\n",
    "    try:\n",
    "        csv_iterator = pd.read_csv(\n",
    "            file_path,\n",
    "            chunksize=chunksize,\n",
    "            usecols=columns,\n",
    "            parse_dates=[date_column],\n",
    "            iterator=True,\n",
    "            delimiter='\\x17',\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_name} not found in {csv_directory}. Skipping.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_name}: {e}\")\n",
    "        return\n",
    "\n",
    "    action_type_column = mapping.get('action_type_column')\n",
    "    action_type_map = mapping.get('action_type_map', {})\n",
    "    default_action_type = mapping.get('action_type', 'Action')\n",
    "\n",
    "    count_of_skips = 0\n",
    "\n",
    "    print(f\"Filtering actions for {len(user_ids)} target users...\")\n",
    "    for chunk in tqdm(csv_iterator, total=int(total_chunks)):\n",
    "        # Drop rows with missing user IDs, dates, or action type columns\n",
    "        dropna_columns = [mapping['user_id_column'], date_column]\n",
    "        if action_type_column:\n",
    "            dropna_columns.append(action_type_column)\n",
    "        chunk.dropna(subset=dropna_columns, inplace=True)\n",
    "\n",
    "        # Ensure data types are consistent\n",
    "        chunk[mapping['user_id_column']] = chunk[mapping['user_id_column']].astype(int)\n",
    "        if action_type_column:\n",
    "            chunk[action_type_column] = chunk[action_type_column].astype(int)\n",
    "\n",
    "        # Filter rows where the user ID is in target_user_ids\n",
    "        filtered_chunk = chunk[chunk[mapping['user_id_column']].isin(user_ids)]\n",
    "\n",
    "        if filtered_chunk.empty:\n",
    "            continue  # Skip if no relevant actions in this chunk\n",
    "\n",
    "        # Prepare action records\n",
    "        records = []\n",
    "        for _, row in filtered_chunk.iterrows():\n",
    "            print(row)\n",
    "            user_id = row[mapping['user_id_column']]\n",
    "            creation_date = row[date_column].isoformat()\n",
    "\n",
    "            if action_type_column:\n",
    "                action_type_id = row[action_type_column]\n",
    "                if action_type_id in action_type_map:\n",
    "                    action_type = action_type_map[action_type_id]\n",
    "                else:\n",
    "                    # print(f\"Unknown action type ID: {action_type_id} in {file_name}. Skipping.\")\n",
    "                    count_of_skips += 1\n",
    "                    continue\n",
    "            else:\n",
    "                action_type = default_action_type\n",
    "\n",
    "            records.append(f\"{user_id},{action_type},{creation_date}\\n\")\n",
    "\n",
    "        # Append records to the consolidated output file\n",
    "        with open(output_file, 'a', encoding='utf-8') as f:\n",
    "            f.writelines(records)\n",
    "\n",
    "    print(f\"Finished processing {file_name}.\")\n",
    "    print(f\"Skipped {count_of_skips} rows with unknown action types.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_users/actions_NonSuperUserIdsSample_0.01.csv\n",
      "Processing Votes.csv...\n",
      "Estimated total chunks: 267\n",
      "Filtering actions for 220533 target users...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77477dfdec44866a6346bda59b22af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CreationDate    2012-11-01 00:00:00\n",
      "UserId                       149316\n",
      "VoteTypeId                        8\n",
      "Name: 2520735, dtype: object\n",
      "CreationDate    2009-02-22 00:00:00\n",
      "UserId                        30480\n",
      "VoteTypeId                        8\n",
      "Name: 4093046, dtype: object\n",
      "CreationDate    2009-08-12 00:00:00\n",
      "UserId                        44482\n",
      "VoteTypeId                        8\n",
      "Name: 6800024, dtype: object\n",
      "CreationDate    2009-02-03 00:00:00\n",
      "UserId                           81\n",
      "VoteTypeId                        8\n",
      "Name: 7477107, dtype: object\n",
      "CreationDate    2009-02-24 00:00:00\n",
      "UserId                        53491\n",
      "VoteTypeId                        8\n",
      "Name: 8701296, dtype: object\n",
      "CreationDate    2018-07-31 00:00:00\n",
      "UserId                       409102\n",
      "VoteTypeId                        8\n",
      "Name: 8837753, dtype: object\n",
      "CreationDate    2011-01-20 00:00:00\n",
      "UserId                       209824\n",
      "VoteTypeId                        8\n",
      "Name: 9010799, dtype: object\n",
      "CreationDate    2009-02-16 00:00:00\n",
      "UserId                        28736\n",
      "VoteTypeId                        8\n",
      "Name: 9064297, dtype: object\n",
      "CreationDate    2009-05-04 00:00:00\n",
      "UserId                        87968\n",
      "VoteTypeId                        8\n",
      "Name: 10438185, dtype: object\n",
      "CreationDate    2009-05-04 00:00:00\n",
      "UserId                      3535708\n",
      "VoteTypeId                        8\n",
      "Name: 11446917, dtype: object\n",
      "CreationDate    2009-04-20 00:00:00\n",
      "UserId                        53491\n",
      "VoteTypeId                        8\n",
      "Name: 11858508, dtype: object\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name, mapping \u001b[38;5;129;01min\u001b[39;00m file_mappings\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     16\u001b[0m     columns \u001b[38;5;241m=\u001b[39m columns_mapping\u001b[38;5;241m.\u001b[39mget(file_name, [mapping[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id_column\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreationDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_super_user_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_super_output_path\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [5], line 44\u001b[0m, in \u001b[0;36mprocess_file\u001b[1;34m(file_name, mapping, columns, user_ids, output_file, chunksize)\u001b[0m\n\u001b[0;32m     41\u001b[0m count_of_skips \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltering actions for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(user_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m target users...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m tqdm(csv_iterator, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(total_chunks)):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Drop rows with missing user IDs, dates, or action type columns\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     dropna_columns \u001b[38;5;241m=\u001b[39m [mapping[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id_column\u001b[39m\u001b[38;5;124m'\u001b[39m], date_column]\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action_type_column:\n",
      "File \u001b[1;32mc:\\Users\\gian\\miniconda3\\envs\\fall24\\lib\\site-packages\\tqdm\\notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gian\\miniconda3\\envs\\fall24\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gian\\miniconda3\\envs\\fall24\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1843\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1843\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1845\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\gian\\miniconda3\\envs\\fall24\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1985\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m   1984\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[1;32m-> 1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gian\\miniconda3\\envs\\fall24\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\gian\\miniconda3\\envs\\fall24\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:921\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1456\u001b[0m, in \u001b[0;36mpandas._libs.parsers._maybe_upcast\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\gian\\miniconda3\\envs\\fall24\\lib\\site-packages\\numpy\\core\\multiarray.py:1131\u001b[0m, in \u001b[0;36mputmask\u001b[1;34m(a, mask, values)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;124;03m    copyto(dst, src, casting='same_kind', where=True)\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \n\u001b[0;32m   1127\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dst, src, where)\n\u001b[1;32m-> 1131\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mputmask)\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mputmask\u001b[39m(a, \u001b[38;5;241m/\u001b[39m, mask, values):\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;124;03m    putmask(a, mask, values)\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1171\u001b[0m \n\u001b[0;32m   1172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, mask, values)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "non_super_file = \"NonSuperUserIdsSample_0.01.parquet\"\n",
    "non_super_sample = pd.read_parquet(f\"../data_users/{non_super_file}\")\n",
    "non_super_user_ids = set(non_super_sample[\"Id\"].astype(int))\n",
    "\n",
    "non_super_output_path = os.path.join(output_directory, \"actions_\" + non_super_file.replace(\".parquet\", \".csv\"))\n",
    "\n",
    "print(non_super_output_path)\n",
    "\n",
    "if os.path.exists(non_super_output_path):\n",
    "    os.remove(non_super_output_path)\n",
    "    with open(non_super_output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"UserId,ActionType,CreationDate\\n\")\n",
    "\n",
    "# Process files for Non-Super Users\n",
    "for file_name, mapping in file_mappings.items():\n",
    "    columns = columns_mapping.get(file_name, [mapping['user_id_column'], 'CreationDate'])\n",
    "    process_file(\n",
    "        file_name=file_name,\n",
    "        mapping=mapping,\n",
    "        columns=columns,\n",
    "        user_ids=non_super_user_ids,\n",
    "        output_file=non_super_output_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Votes.csv...\n",
      "Estimated total chunks: 267\n",
      "Filtering actions for 22076 target users...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3900c8c111b94deeb018c7f457b34757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing Votes.csv.\n",
      "Skipped 63554 rows with unknown action types.\n",
      "\n",
      "Processing Posts.csv...\n",
      "Estimated total chunks: 1015\n",
      "Filtering actions for 22076 target users...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3377b5a4a94429888ee4dd4bc3b7833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing Posts.csv.\n",
      "Skipped 47408 rows with unknown action types.\n",
      "\n",
      "Processing Comments.csv...\n",
      "Estimated total chunks: 95\n",
      "Filtering actions for 22076 target users...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b774d3530f4dd880b469dbd3077132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing Comments.csv.\n",
      "Skipped 0 rows with unknown action types.\n",
      "\n",
      "Processing Badges.csv...\n",
      "Estimated total chunks: 56\n",
      "Filtering actions for 22076 target users...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f941582af14ad49e8a7ad432ce7596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing Badges.csv.\n",
      "Skipped 0 rows with unknown action types.\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# # Define output file paths\n",
    "# super_output_path = os.path.join(output_directory, \"super_users_actions.csv\")\n",
    "\n",
    "# # remove the output paths if they already exist\n",
    "# if os.path.exists(super_output_path):\n",
    "#     os.remove(super_output_path)\n",
    "\n",
    "# # Process files for Super Users\n",
    "# for file_name, mapping in file_mappings.items():\n",
    "#     columns = columns_mapping.get(file_name, [mapping['user_id_column'], 'CreationDate'])\n",
    "#     process_file(\n",
    "#         file_name=file_name,\n",
    "#         mapping=mapping,\n",
    "#         columns=columns,\n",
    "#         user_ids=super_user_ids,\n",
    "#         output_file=super_output_path\n",
    "#     )\n",
    "\n",
    "super_user_file = \"SuperUserIds.parquet\"\n",
    "super_user_sample = pd.read_parquet(f\"../data_users/{super_user_file}\")\n",
    "super_user_ids = set(super_user_sample[\"Id\"].astype(int))\n",
    "super_output_path = os.path.join(output_directory, \"actions_\" + super_user_file.replace(\".parquet\", \".csv\"))\n",
    "\n",
    "# remove the output paths if they already exist\n",
    "if os.path.exists(super_output_path):\n",
    "    os.remove(super_output_path)\n",
    "\n",
    "# Process files for Super Users\n",
    "for file_name, mapping in file_mappings.items():\n",
    "    columns = columns_mapping.get(file_name, [mapping['user_id_column'], 'CreationDate'])\n",
    "    process_file(\n",
    "        file_name=file_name,\n",
    "        mapping=mapping,\n",
    "        columns=columns,\n",
    "        user_ids=super_user_ids,\n",
    "        output_file=super_output_path\n",
    "   )\n",
    "    \n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
